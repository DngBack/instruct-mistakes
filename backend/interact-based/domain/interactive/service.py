from __future__ import annotations

from infra.llm_service import OpenAIInput
from infra.llm_service import OpenAIService
from shared.logging import get_logger
from shared.models import ExercisesType

from .base import InteractionInput
from .base import InteractionOutput
from .base import InteractionService
from .prompt import INTERACTION_SYSTEM_PROMPT
from .prompt import INTERACTION_USERS_PROMPT

logger = get_logger(__name__)


class LLMInteractionService(InteractionService):
    """
    Service class to handle interactions using an LLM model.
    """

    llm_model: OpenAIService

    def process(self, inputs: InteractionInput) -> InteractionOutput:
        """
        Process user input and generate a response using the LLM model.

        Args:
            inputs (InteractionInput): The input data containing user answer, correct answer, and exercise type.

        Returns:
            InteractionOutput: The output containing the generated response.
        """
        user_prompt = self._get_user_inputs(
            user_answer=inputs.user_answer,
            correct_answer=inputs.correct_answer,
            exercies_type=inputs.exercies_type,
        )
        system_prompt = INTERACTION_SYSTEM_PROMPT
        reponse = self._generate_response(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
        )
        return InteractionOutput(response=reponse)

    def _get_user_inputs(
        self,
        user_answer: str,
        correct_answer: str,
        exercies_type: ExercisesType,
    ) -> str:
        """
        Generate a formatted user prompt based on provided answers and exercise type.

        Args:
            user_answer (str): The answer provided by the user.
            correct_answer (str): The correct answer.
            exercies_type (ExercisesType): The type of exercise.

        Returns:
            str: The formatted user prompt.
        """
        user_prompt = INTERACTION_USERS_PROMPT.replace(
            '{exercise_type}',
            str(exercies_type.value),
        )
        user_prompt = user_prompt.replace('{user_answer}', user_answer)
        user_prompt = user_prompt.replace('{correct_answer}', correct_answer)
        return user_prompt

    def _generate_response(self, user_prompt: str, system_prompt: str) -> str:
        """
        Generate a response from the LLM model.

        Args:
            user_prompt (str): The user-generated prompt.
            system_prompt (str): The system-level prompt.

        Returns:
            str: The response generated by the LLM model.
        """
        response = self.llm_model.process(
            OpenAIInput(user_prompt=user_prompt, system_prompt=system_prompt),
        )
        return response.response
